---
title: "TD2 à rendre"
author: "Erwan Judic, Barbier--Darnal Joseph"
output:
  html_document: default
  pdf_document: default
---

```{r echo=TRUE, message=FALSE, warning=FALSE}
rm(list=ls())
setwd("/Users/josephbarbier/Desktop/M1S2/stats computationnelles")
library(FactoMineR)
library(factoextra)
library(stats)
library(ggplot2)
library(hrbrthemes)
library(RColorBrewer)
library(maps)
library(ClustOfVar)
library(ggpubr)
```






# Exercice 2

### 1 - Open file

```{r}
df = read.table("fromage.txt", header = TRUE, row.names = 1)
head(df)
```





\
<br>

### 2 - Standardize data

```{r echo=TRUE, message=FALSE, warning=FALSE}
#uncorrected variance
n = nrow(df)

#scale
df_norm = scale(df) * sqrt(n/(n-1))
df_norm = as.data.frame(df_norm)
```



\
<br>

### 3 - Ward's CAH

```{r echo=TRUE, message=FALSE, warning=FALSE}
d = dist(df_norm)
tree = hclust(d^2/(2*n), method="ward.D")
k = 5
group_labels_ward = cutree(tree, k = k)
df_norm$group = group_labels_ward
```




\
<br>

### 4 - Explained inertia

```{r echo=TRUE, message=FALSE, warning=FALSE}
explained_inertia = sum(sort(tree$height, decreasing = TRUE)[1:k]) / sum(tree$height) *100
cat("Percentage of explained inertia (ward's CAH, with k =", k, ")", round(explained_inertia,4), "%")
```




\
<br>

### 5 - K-means

```{r echo=TRUE, message=FALSE, warning=FALSE}
k_means = function(k, df=df_norm[1:9], nstart=1){
  kmeans = kmeans(df, k, nstart=nstart)
  explained_inertia = kmeans$betweenss/kmeans$totss*100
  cat("Percentage of explained inertia (k-means, with k =", k, ")", round(explained_inertia,2), "%\n")
}

k_means(5)
```





\
<br>

### 6 - Multiple k-means

```{r echo=TRUE, message=FALSE, warning=FALSE}
for (i in rep(1,5)){
  k_means(5)
}

```

Le pourcentage d'inertie expliquée change en re-exécutant l'algorithme des k-means, pour un k fixe. Cela est dû au fait que la première partition, à l'étape 1 de l'algorithme, est aléatoire, amenant l'algorithme à converger vers différents extrema locaux en fonction de cette initialisation.



\
<br>

### 7- N-start argument

Afin de trouver un *meilleur* extremum local, on peut fixer la valeur de l'argument `nstart` en la mettant plus élevée. En faisant cela, on fait tourner l'algorithme *nstart-*fois et on garde la partition avec la plus grande proportion d'inertie expliquée. Cette argument de la fonction kmeans permet de faire tourner plusieurs fois l'algorithme via une seule ligne de code.  






\
<br>

### 8 - Best clustering method

Dans notre cas, la meilleure méthode de partitionnement en 5 classes est celle de la CAH de Ward puisqu'elle maximise davantage notre critère d'inertie inter-classe (79.6% vs 77% au mieux pour les kmeans). 





\
<br>

### 9 - Table

```{r echo=TRUE, message=FALSE, warning=FALSE}
kmean = kmeans(df_norm[1:9], 5, nstart=10)
group_labels_kmeans = kmean$cluster
table(group_labels_kmeans, group_labels_ward)
```

La fonction `table` nous permet de voir la différence de réparition parmi les deux méthodes de partitionnement utilisée. Dans notre cas, on retrouve 2 individus qui diffèrent entre les partitions proposées. Dit autrement : la partition en 5 classes, quelle soit déterminée par l'algorithme de Ward ou celui des k-means, est fortement similaire. 











\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>

# Exercice 3


### 1 - Ward's CAH

```{r echo=TRUE, message=FALSE, warning=FALSE}
load("protein.rda")
n = nrow(protein)

protein_scale = scale(protein)*sqrt(n/(n-1))
protein_scale = as.data.frame(protein_scale)
d = dist(protein_scale)
arbre = hclust(d^2/(2*n), method = "ward.D")
sum(arbre$height)
```

La somme des hauteurs est bien égal à l'inertie total ainsi qu'au nombre de variables : 9.




\
<br>

### 2 - Explained inertia

```{r echo=TRUE, message=FALSE, warning=FALSE}
k = 3
explained_inertia = sum(sort(arbre$height, decreasing = TRUE)[1:k]) / sum(arbre$height) *100
cat("Percentage of explained inertia (Ward's CAH, with k =", k, ")", round(explained_inertia,2), "%\n")
```




\
<br>

### 3 - Kmeans

```{r echo=TRUE, message=FALSE, warning=FALSE}
part = cutree(arbre, k = k)
protein_scale$cluster = part

Z1 = protein_scale[part == 1,]
Z2 = protein_scale[part == 2,]
Z3 = protein_scale[part == 3,]

g1 = apply(Z1, 2, mean)
g2 = apply(Z2, 2, mean)
g3 = apply(Z3, 2, mean)

centroids = rbind(g1, g2, g3)
centroids
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
#spc = split(protein_scale, as.factor(part), drop = FALSE)
km = kmeans(protein_scale,centers=centroids,nstart = 10)
km$cluster
```






\
<br>

### 4 - Table

```{r echo=TRUE, message=FALSE, warning=FALSE}
partWard= cutree(arbre,k=3)
table(partWard,km$cluster) 
```

La plupart des individus ont été placés dans les mêmes clusters, sauf 2. 




\
<br>

### 5 - Explained inertia

```{r echo=TRUE, message=FALSE, warning=FALSE}
explained_inertia = km$betweenss/km$totss*100
cat("Percentage of explained inertia (k-means, with k =", k, ")", round(explained_inertia,2), "%\n")
```

Le pourcentage d'inertie expliqué est désormais de 52% avec la consolidation des k-means, soit une amélioration d'environ 4 points de pourcentage.

```{r fig.align='center', fig.width=12, echo=TRUE, message=FALSE, warning=FALSE}
object = list(data = protein_scale[1:9], cluster = partWard)
plot1 = fviz_cluster(object, data = protein_scale[1:9], stand = FALSE, main = "Cluster plot partition de Ward")
plot2 = fviz_cluster(km, data = protein_scale[1:9], stand = FALSE, main = "Cluster plot consolidation des k-means")

ggarrange(plot1, plot2, ncol = 2)
```

On observe alors que deux pays ont changés de classes, l'URSS et la Hongrie. Ce résultat est obtenu via les graphiques et sur le tableau croisé précédent. 













\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>

# Exercice 4

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = read.table("urbanGB.txt", sep = ",", dec=".", header = FALSE)
colnames(df) = c("longitude", "latitude")
head(df)
```


\
<br>

### 1 - Scatter plot

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.width=4, fig.height=5}
ggplot(df, aes(x = longitude, y = latitude)) + geom_point() + theme_classic() + ggtitle("Accidents en Angleterre")
```





\
<br>

### 2 - Dimensionality reduction with k-means clustering

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.width=6, fig.height=7}
#kmeans and centroids
kmeans_result = kmeans(df[,c("longitude","latitude")], centers=1000, iter.max=20)
centers = aggregate(df[,c("longitude","latitude")], by = list(cluster = kmeans_result$cluster), FUN = mean)
centers$weight = kmeans_result$size / nrow(df)

#plot new graph
ggplot(centers, aes(x = longitude, y = latitude)) + theme_classic() + ggtitle("1000 accidents moyens") + geom_point()
```




\
<br>

### 3 - Ward's algorithm

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.width=4, fig.height=5}
#ward algorithm
dist_matrix = dist(centers[, c("longitude", "latitude")])
n = nrow(centers)
ward_result = hclust(dist_matrix^2/(2*n), method = "ward.D")
partition = cutree(ward_result, k = 10)

#plot new graph
ggplot(centers, aes(x = longitude, y = latitude)) + geom_point(col=partition) + theme_classic() + ggtitle("1000 accidents moyens")
```



\
<br>

### 4 - England map

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.width=4, fig.height=5}
worldmap = map_data('world')
carte = ggplot() +
  geom_polygon(data = worldmap, aes(x = long, y = lat, group = group),
               fill = 'gray90',color = 'black') +
  coord_fixed(ratio = 1.3,  xlim = c(-10,3), ylim = c(50, 59)) +
  theme_void()
carte
```







\
<br>

### 5 - Map + partition

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.width=7, fig.height=8}
carte = ggplot() +
  geom_polygon(data = worldmap, aes(x = long, y = lat, group = group),
               fill = 'gray90',color = 'black') +
  coord_fixed(ratio = 1.3,  xlim = c(-10,3), ylim = c(50, 59)) +
  theme_void() +
  geom_point(data = centers, aes(x = longitude, y = latitude, color = as.factor(partition)), alpha = 0.6, size = 1.5)
carte
```


On retrouve ici l'idée intuitive que les (centroids d') accidents les plus proches au sens de Ward correspondent aux (centroids d') accidents les plus proches au sens géographique : la position des individus dans l'espace des paramètres est littéralement la latitude et longitude réelle. 

On remarque également des clusters parmi les clusters. La position des groupes définis par la partition de Ward est graphiquement bien distincte. En effet, on peut aisément repérer de grandes villes au centre des clusters de la partition, notamment Londres, Manchester, Birmingham, Durham, Edimbourg/Glasgow. Il semble donc qu'il y ait plutôt davantage d'accidents proche ou au sein des grandes villes de Grande-Bretagne, dans l'hypothèse où les accidents éloignés des villes soient documentés dans le jeu de données initial. 






<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\

# Exercice 5


```{r echo=TRUE, message=FALSE, warning=FALSE}
data(wine)
print(wine[1:5,c(1:2,27,26,29)])
```

### 1 - FAMD

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
res = FAMD(wine)
```

Le premier graphique représente la projection des individus aux 2 premiers axes factoriels avec les modalités des variables catégoriels en rouges. Le deuxième graphique représente la projection des individus sur les deux premiers axes factoriels en gardant uniquement les variables numériques. Le troisième graphique est la projection des variables sur les deux premiers axes factoriels, avec encore une fois les variables catégoriels en rouge. Le quatrième graphique est la projection de la modalité que peuvent prendre les individus sur les 2 premiers axes factoriels. Le cinquième est le cercle de corrélation des variables numériques sur les deux premiers axes factoriels.

Contribution des variables au premier axe factoriel :

```{r echo=TRUE, message=FALSE, warning=FALSE}
contrib = abs(res$var$coord * sqrt(res$eig[1:5]))
contrib1 = contrib[,1]
contrib1 = sort(contrib1, decreasing = TRUE)/sum(contrib1)*100

#top 10 contributions to the first axis
round(contrib1, digits = 2)[1:10]
```

Ainsi une grande partie des variables contribuent légèrement au premier axe factoriel de l'ACP mixte, avec Intensity contribuant le plus fortement.

Contribution des variable pour le second axe factoriel :

```{r echo=TRUE, message=FALSE, warning=FALSE}
contrib2 = contrib[,2]
contrib2 = sort(contrib2, decreasing = TRUE)/sum(contrib2)*100

#top 10 contributions to the second axis
round(contrib2, digits = 2)[1:10]
```

On observe que les variables spice before shaking, le type de sol, l'amertume et l'intensité olfactive avant mélange contribuent le plus fortement au deuxième axe, et sont également des variables contribuant peu au premier axe facotriel.


\
<br>

### 2 - Maximum number of principal component

Nous avons au maximum 20 composantes principales, égal au nombres d'observations - 1.

```{r  echo=TRUE, message=FALSE, warning=FALSE}
res2 = FAMD(wine, ncp = 20, graph = FALSE)
```




\
<br>

### 3 - Ward's CAH

```{r message=FALSE, warning=FALSE}
result_hcpc = HCPC(res2,nb.clust = 4, graph = FALSE, consol = FALSE)
fviz_dend(result_hcpc, main = "CAH sur les 20 premières composantes principales", k_colors = "black", rect = TRUE, rect_border = "red", rect_lty = 1)
```




\
<br>

### 4 - Kmeans consolidation

```{r message=FALSE, warning=FALSE}
result_hcpc2 = HCPC(res2, nb.clust = 4, graph =FALSE, consol = TRUE)

part_Ward = result_hcpc$data.clust$clust
part_km = result_hcpc2$data.clust$clust

table(part_km,part_Ward)
```

Grace à la fonction `table`, on observe que deux vins ont changés de classes.




\
<br>

### 5 - Interpretation

Interprétation des 4 classes de vins.

Sur le premier graphique on observe que les individus du cluster3 auront des valeurs plus élevés, donc supérieur à la moyenne, pour les variables les mieux réprésentés par les variables du premier axe factoriel. Ces résultats se retrouvent également sur le deuxième graphique.

Les individus du cluster2 ont des valeurs très faibles pour les variables du premier axe facotriel.

Les individus du cluster4 n'ont pas vraiment de valeurs supérieur à la moyenne pour toutes les variables du jeu de donnée.

Pour finir les individus du cluster 4 ont des valeurs très élevé pour les variables du second axe factoriel.







\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
\
<br>
